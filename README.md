Sentiment Analysis with Knowledge Distillation

This project performs sentiment analysis using a Teacher-Student model architecture in Google Colab. The student model is trained through knowledge distillation techniques, where it learns from the teacher model’s reasoning behind its predictions.

Key Concepts:

Teacher Model: A pre-trained model (e.g., Groq’s Deepseek R1, or similar) used to generate reasoning for sentiment predictions.

Student Model: A smaller model (e.g., BERT) trained to classify sentiment based on both the reasoning and the sentiment labels.

Knowledge Distillation: The process where the student learns from both the sentiment labels and the teacher’s reasoning.

Project Overview-

The project involves training a Student Model with:

Sentiment Labels: Direct sentiment classification.

Reasoning: Step-by-step explanations generated by the Teacher Model.

The student model is trained using Hugging Face's Transformers library in Google Colab, leveraging PyTorch for model training and Hugging Face's Datasets for managing the training data.

Technologies Used -

Google Colab: For executing the project in an online environment.

Hugging Face Transformers: For the Teacher and Student models.

PyTorch: Deep learning framework used for model training.

Groq API (or another reasoning model): For generating step-by-step reasoning.

Langchain (optional): For enhanced integration with models.

Accelerate: For optimized training using GPUs.

Datasets: For handling and preprocessing datasets.

Setup and Installation
Clone the repository (if applicable): If you have a GitHub repo, you can clone it to Colab directly:


!git clone https://github.com/yourusername/sentiment-analysis-distillation.git
%cd sentiment-analysis-distillation
Install required libraries: You can directly install dependencies in Colab by running:


!pip install transformers datasets torch accelerate langchain
Set up Groq API (if using for reasoning): You can provide your Groq API key by running the following:


import os
os.environ['GROQ_API_KEY'] = 'your_api_key'
Data
The SST-2 (Stanford Sentiment Treebank) dataset is used, containing movie reviews labeled as either positive or negative.

Sentiment labels are used directly in the training process.

Reasoning for each sentiment label is generated by the Teacher model to aid the Student model's learning.

Training the Models
Teacher-Student Model Architecture
Teacher Model generates reasoning for a given input.

Student Model is trained on both the sentiment label and reasoning generated by the teacher.

Training Script Example
Here’s an example script you can run in Google Colab to train your model:


# Training script in Google Colab

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Example of loading a dataset

dataset = load_dataset('glue', 'sst2')

# Set up Teacher and Student models (using pre-trained models)

teacher_model = AutoModelForSequenceClassification.from_pretrained('groq_model')  # replace with your teacher model
student_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')

# Define training arguments

training_args = TrainingArguments(

    output_dir='./results',          # output directory
    num_train_epochs=3,              # number of training epochs
    per_device_train_batch_size=8,   # batch size for training
    per_device_eval_batch_size=8,    # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    evaluation_strategy="epoch",     # evaluation strategy to adopt during training
    save_strategy="epoch",           # save strategy during training
    
)

# Define the Trainer

trainer = Trainer(

    model=student_model,                         # the model to be trained
    args=training_args,                          # training arguments
    train_dataset=dataset['train'],              # training dataset
    eval_dataset=dataset['validation'],          # evaluation dataset
    tokenizer=tokenizer                          # tokenizer for text preprocessing
    
)

# Train the model
trainer.train()

Evaluation

After training the student model, it can be evaluated by predicting the sentiment of new inputs. Here’s an example function for evaluation:

import torch

def predict_sentiment(input_text):

    input_tokens = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = student_model(**input_tokens)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=-1).item()
    
    return predicted_class

# Test with sample input

input_text = "I am very happy today!"
predicted_class = predict_sentiment(input_text)
print(f"Predicted Sentiment: {predicted_class}")

Training Logs and Metrics-

Training Loss: Logs the loss during training.

Validation Loss: Evaluates the performance after each epoch on the validation dataset.

Example of training logs:

Epoch 1: Training Loss - 0.522523 | Validation Loss - 0.530785
Epoch 2: Training Loss - 0.512523 | Validation Loss - 0.419531
Epoch 3: Training Loss - 0.501223 | Validation Loss - 0.347946

Conclusion:

The model performs sentiment analysis by not only predicting sentiment labels but also learning from the reasoning generated by the teacher model. This approach enhances the student model's understanding and improves its predictions.
