{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FO7vpslfNf0N",
    "outputId": "f391bc4d-38a9-4cca-a4cb-5d84aea05d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HIHTuNOSN-iu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "6ad2b674cdac4a5fb13c28b888a73087",
      "b2e8122df8fa4e4c87c939c227e6953d",
      "ad66068dd4fe4a139e2b93c802e50024",
      "ae4c2d213bc14c388392b64cb38dc0a9",
      "15a4608fd7e0467fb798a455fdecdd64",
      "f11c0985419540529ef0a401a4a68944",
      "b327e1ca9b5e4914953ac93593430e7e",
      "021d07900a9649b0a07753e89ea4449a",
      "38f128b6ff3141299a969843b60b8a2f",
      "859d7a678e914f2c98b31aa87bc43031",
      "2c577f5c61994c899ae8d331951ce733",
      "98c7321b735747f09ae0120df133de14",
      "c0a76be58ba84bb6801312f62ae3b3fc",
      "664a38fb41ab4e36bcd1b39443b3e6a2",
      "7f62ab6d83b64598a4ef23ae49176991",
      "9c6f3ac0d03a4ffba37ee54bf70bde18",
      "050d83dd2e534ca5a67657584675243e",
      "d8d195ce21c0468ca2d33be1bfa55245",
      "755c3ab7b91b4337bfad5b6aecf0a7ae",
      "810ddfe7599f4a4389e05298201204fc",
      "6f7050f65b47491faa04f0512e6c3746",
      "6ea5e1b73b264a679cf3659d37283a09"
     ]
    },
    "id": "5-MdLjh12Hpr",
    "outputId": "6fef9cdf-d348-43ba-eebb-8994a6850bc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2b674cdac4a5fb13c28b888a73087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c7321b735747f09ae0120df133de14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "student_llm_model = AutoModelForSequenceClassification.from_pretrained(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enHqkHew2x7h",
    "outputId": "915ba192-5db4-4cc4-b52b-d70eeccab418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(student_llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "63842222b55e40cfb68e9e16f3e4c635",
      "754305b9db3049eca6cb5f1a12578294",
      "262ca6892b704fa8b3702472f36c8e80",
      "2cd52ca43cc8472fa3e23d09df4b963c",
      "039dc548cba549759375da8d37146dba",
      "bee51eb8f10241648d15659d450962b9",
      "5f7b44799bf24712a5c6c006c419fc20",
      "a0b4e6d12a8247e781102b81c317207c",
      "1ded9bd4dfaf4cb09dda0446360be66f",
      "889de1e563d84b119348c44e64af2e65",
      "2db1246484374e5ca543ab55f892a3c5",
      "ee152d2197804e3c8a5b61e16d4e3b8a",
      "3d862dc81b354154b1b5fedb22611d73",
      "248779ce392d42cc95c8c55fd3d5c975",
      "ad38f9f7f2274866a65dcab3783a67ef",
      "213fbb1315c142dea093fac722fb3511",
      "9aa088368a994533b67bf3691dd20bed",
      "8d7fb90814604ebdaa07931d091e0870",
      "3f1dc42ddb584ecab66b8d08910f6232",
      "8bcb61330ac342ff8f17b335c68a7dbb",
      "4ff246fd3a684cae947f98e29f810cd3",
      "fcc3dd5193214bcb9a2fe6086c6e9177",
      "2d632a523dfc401480259b14f40d108e",
      "a804f851429044079c97542e3b65b416",
      "2f02217aa36c4c9a9334fe04ab938712",
      "2b23ab300e004baab7477f5d4d33faad",
      "ca25d9b8d78749908cfbd07ec6d6d874",
      "3f9a65134f044216a44bb738f591eddd",
      "9e6a38f500c148fda03427d77e9ef6b4",
      "fdfae688597d42bd8eca278eb7d6c1a8",
      "2504d1354ab847939e252ea0e2bb1b3f",
      "e253d246348e4961989e68e3add7ee88",
      "80d9d449d4d54aa9968e93ab4e4c16ae",
      "779259ba5e304bff8ceffd237ab4242b",
      "706046e3cf1040ef95c0d811cf31bba1",
      "1a7fff24f9654c9f83d83261176419c9",
      "fa87bf14453f4d4398f5b83db5a485e9",
      "5ae946c1ab984fd5af5f64eb2a1dfefe",
      "740faaf7bf5342c9850d92fe2cdb7c3c",
      "a7ff10af554e440681d9ac4f80fe8cb6",
      "1d1fb27e7da548d8b5abacc97f9f7824",
      "b9598aebd88e49839b1611a438693073",
      "dfc7280dff4b40cdba81e5b0363485d8",
      "97b07d62d4a947bb8b7fc15b2a6e2907",
      "f6f3f4b32f8d4b28aa7095f6e6e8ca66",
      "f791978bbd0b4684b036738645f7c5c8",
      "baaffe2319d34919b9c511721e8350b2",
      "8d544c06ef9d44ad9986c76c4a915aac",
      "b0056362806f471a94d5289b8a7f23aa",
      "bb50503665a247c0b9b4f6e3574a55bc",
      "15833219ab5b4635b5ff7c4f8194a11f",
      "9ea51bdea969471d81dbc8ab1767fd58",
      "973d75f6eacd443d8c3f4b5ec9f304e3",
      "4a77a6ff5f6e4f1ba653594e903581f8",
      "2d3c5d35a5184f20992775d8460953e1",
      "5b9cae99a8aa4756b1b328f94e435cf4",
      "aca19dd5949f4ba6b1ce7c2107a3fd3a",
      "8c17a3104ea846c486f190e37e8c1f54",
      "134be0adef224187a1731a51df6a4e5f",
      "4a84287be36744538e3126cf51d25196",
      "b7a62da2d507406e8b49dd2e9bd0fb87",
      "5db8bde13cff417bbcb26ec3161c3131",
      "0edddbada97045da89f606b2380aa2e5",
      "d7d28ce531f04550973a8b15f9e10ae7",
      "6744a0488bc347ccb41745e675a80bcc",
      "3bb122cd02c74945948fdbd295c2f7bf",
      "b76e0d286417427f8f0b3413f5fb73be",
      "4b1abb6438f64dea9bb4b7021434625e",
      "6c28796db9104aa0ac58ff46f4babede",
      "0695cc94db9a40d7963935164228d58a",
      "87768a72225a49fbb0c530abe1aeac80",
      "f29295fdeefd4be9b3f8f4390abc20ab",
      "cba5074028384199a45f6a08ecb555cf",
      "9545c0093a6040a6a846927896acb2c6",
      "690721fbd247488893782606e0f98843",
      "9f3278f966d548918f516c4f08f47cf3",
      "3b9e2a840ab54f5ea632084c2df501a8"
     ]
    },
    "id": "J1aZFTFo3J6h",
    "outputId": "d10568a4-b3f0-4230-cdc3-29631137b5dd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63842222b55e40cfb68e9e16f3e4c635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee152d2197804e3c8a5b61e16d4e3b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d632a523dfc401480259b14f40d108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779259ba5e304bff8ceffd237ab4242b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f3f4b32f8d4b28aa7095f6e6e8ca66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9cae99a8aa4756b1b328f94e435cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e0d286417427f8f0b3413f5fb73be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\",split = \"train[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Psxvm2JG4Ubo",
    "outputId": "c4a0f4c7-0b2f-429d-f2a5-c45b207a3751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence', 'label'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQqGPIMo4ag2",
    "outputId": "2ee4a97d-461f-4054-bf0a-fb41963a1c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'sentence': 'hide new secretions from the parental units ', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b02C3CzI88dA",
    "outputId": "4acfd188-6d46-4d7a-98c7-da1ae5f59dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.4.7\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.3.51)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.3.24)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (4.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.11.2)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Downloading langchain_ollama-0.3.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: langchain-ollama\n",
      "Successfully installed langchain-ollama-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "\n",
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A1qsNurq-cLk"
   },
   "outputs": [],
   "source": [
    "#from langchain_ollama import ChatOllama\n",
    "\n",
    "#llm_chat_engine = ChatOllama(\n",
    "                             #model=\"deepseek-r1:7b\",\n",
    "                             #base_url=\"127.0.0.1:11434\",\n",
    "                             #temperature=0.3\n",
    "#)\n",
    "\n",
    "#test_prompt = \"I am very joyful today\"\n",
    "\n",
    "#student_llm_response = llm_chat_engine.invoke(test_prompt)\n",
    "\n",
    "#print(student_llm_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvZAullCSpGM",
    "outputId": "7fa2bcb8-a69f-4d21-bf1e-b9362318ee33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.51)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
      "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.24)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.3.0)\n",
      "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
      "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
      "Successfully installed groq-0.22.0 langchain-groq-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "\n",
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "370kNYwJTQe9"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = userdata.get('LLMProjectKey1')\n",
    "\n",
    "teacher_model = ChatGroq(\n",
    "\n",
    "                         model = \"deepseek-r1-distill-llama-70b\",\n",
    "\n",
    "                         temperature = 0.3\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEe2snuBUiWF",
    "outputId": "645c97a0-8e9e-452b-efc6-fa72b1572a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out why the user said they're very joyous today. They mentioned feeling happy and asked how I am. I responded by saying I'm just a computer program without emotions, but I'm here to help. Now, I need to think through how to approach this.\n",
      "\n",
      "First, I should consider the user's emotions. They're feeling joyful, so maybe they want to share something positive or just chat in a friendly manner. I should acknowledge their happiness and perhaps ask what's making them feel that way. It's important to show empathy and interest.\n",
      "\n",
      "I also need to maintain the boundaries of being an AI. I can't feel emotions, but I can certainly be supportive and helpful. Maybe I can offer assistance in some way, like suggesting activities they might enjoy or helping them with something they're happy about.\n",
      "\n",
      "I should think about possible reasons why someone might feel joyous. It could be a special event, an achievement, good news, or just a general positive mood. By asking open-ended questions, I can help them elaborate and feel heard.\n",
      "\n",
      "Additionally, I should keep the conversation positive and uplifting. Maybe share some general tips on maintaining joy or suggest ways to make the most of their good day. It's all about being supportive without overstepping.\n",
      "\n",
      "I also need to be cautious not to make assumptions. Instead of assuming why they're happy, I should let them share if they feel comfortable. This approach respects their privacy and allows them to guide the conversation.\n",
      "\n",
      "In summary, my response should be empathetic, supportive, and engaging. I'll acknowledge their joy, express interest in their well-being, and offer help if needed. Keeping the tone positive and friendly will help maintain a constructive dialogue.\n",
      "</think>\n",
      "\n",
      "I'm delighted to hear about your joyful day! It's wonderful that you're feeling so positive. If you're comfortable sharing, what's bringing you such happiness today? Whether it's a special event, an achievement, or just a great mood, I'm here to listen and help in any way I can. Let's make the most of your day together!\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"I am very joyous today\"\n",
    "\n",
    "teacher_model_response = teacher_model.invoke(test_prompt)\n",
    "\n",
    "print(teacher_model_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kjCwJOyIVDqN"
   },
   "outputs": [],
   "source": [
    "def generate_model_reasoning(input_text):\n",
    "\n",
    "  \"\"\"\n",
    "  Uses Groq's Deepseek R1 Model to generate a step by step Rationale(Reasoning) for the Output given for particular Input(User prompt)\n",
    "  \"\"\"\n",
    "\n",
    "  user_prompt = f\"Deliver step by step reasoning before responding to: {input_text}\" #Chain of thoughts prompting request for Step by Step Distillation\n",
    "\n",
    "  teacher_model_response = teacher_model.invoke(user_prompt)\n",
    "\n",
    "  return teacher_model_response.content if hasattr(teacher_model_response, \"content\") else teacher_model_response\n",
    "\n",
    "teacher_model_response = generate_model_reasoning(\"Explain AI Agents and the technologies used behind the same, keep in mind it should be widely adopted in Industry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mPdBr2-YIPq",
    "outputId": "cb172b0c-4e32-4682-bc58-e623fb8f5453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain AI agents and the technologies behind them, especially focusing on how they're widely adopted in industries. Hmm, where do I start? I remember that AI agents are like autonomous entities that can perform tasks on their own, but I'm not entirely sure about the specifics. Let me break this down.\n",
      "\n",
      "First, what exactly is an AI agent? I think it's a program that uses AI to make decisions and act without human intervention. They can interact with environments, right? So, maybe they can be in software, robots, or even as chatbots. I should probably define that clearly.\n",
      "\n",
      "Next, the core characteristics. I recall that autonomy is a big one—AI agents operate on their own. Then there's reactivity, meaning they respond to changes. Proactivity might be another, where they take initiative. Social ability could be interacting with other agents or humans. And maybe adaptability, learning from experiences. I should list these out.\n",
      "\n",
      "Now, the technologies behind AI agents. I know machine learning is a key part. There's supervised, unsupervised, and reinforcement learning. Each has its role. For example, reinforcement learning is used in game AI like AlphaGo. Natural Language Processing (NLP) is another, important for chatbots and understanding human language. Computer Vision deals with images and videos, so that's used in things like facial recognition or autonomous cars.\n",
      "\n",
      "Then there's the concept of an environment. I think the environment is where the agent operates. It could be virtual, like software, or physical, like robots. The agent interacts with this environment, either by perceiving it through sensors or acting through effectors. I should explain the types of environments too—static or dynamic, deterministic or stochastic. Maybe episodic versus continuous as well.\n",
      "\n",
      "Applications in industries. Let's see, customer service uses chatbots. Healthcare could use virtual assistants or diagnostic tools. In manufacturing, autonomous robots handle tasks. Finance uses agents for trading or fraud detection. Transportation has self-driving cars. Supply chain uses agents for logistics. I should give examples for each industry to make it clear.\n",
      "\n",
      "Challenges and limitations. Data privacy is a big issue, especially with regulations like GDPR. Bias in AI is another problem. Explainability is important for trust. Scalability and performance are technical challenges. Ethical considerations like job displacement and accountability are also key points. I need to address each of these.\n",
      "\n",
      "Real-world examples would help. IBM Watson Assistant is a good one for customer service. In healthcare, maybe a virtual nurse assistant. Autonomous robots like those from Kuka in manufacturing. In finance, something like robo-advisors. For transportation, Tesla Autopilot. In supply chain, systems like SAP's agents. These examples make the explanation more concrete.\n",
      "\n",
      "Looking ahead, future trends might include edge AI, where agents process data locally. Multi-agent systems, where agents collaborate. Explainable AI (XAI) for transparency. Human-AI collaboration for more effective teamwork. These trends show where AI agents are heading.\n",
      "\n",
      "In conclusion, AI agents are versatile and widely adopted because they can automate tasks, provide 24/7 service, and scale operations. They use technologies like ML, NLP, and Computer Vision. But there are challenges like data privacy and bias that need addressing.\n",
      "\n",
      "Wait, did I miss anything? Maybe the difference between different types of learning—supervised, unsupervised, reinforcement. Also, ensuring that each section flows logically. I should make sure each part is explained clearly without getting too technical. Maybe I should also mention the environment types in more detail. Oh, and the challenges section needs to cover why these are issues and how they impact adoption.\n",
      "\n",
      "I think that's a solid structure. Now, I just need to put it all together in a coherent way, making sure each part is explained step by step so someone without a technical background can understand.\n",
      "</think>\n",
      "\n",
      "**Explanation of AI Agents and Their Technologies**\n",
      "\n",
      "**Introduction to AI Agents:**\n",
      "AI agents are autonomous programs designed to perform tasks without human intervention. They operate in various environments, including software applications, robots, and chatbots, making decisions and acting based on their programming and interactions.\n",
      "\n",
      "**Core Characteristics of AI Agents:**\n",
      "1. **Autonomy:** Operate independently without human control.\n",
      "2. **Reactivity:** Respond to environmental changes in real-time.\n",
      "3. **Proactivity:** Take initiative to achieve goals.\n",
      "4. **Social Ability:** Interact with humans and other agents.\n",
      "5. **Adaptability:** Learn from experiences to improve performance.\n",
      "\n",
      "**Technologies Behind AI Agents:**\n",
      "1. **Machine Learning (ML):** Enables agents to learn from data through supervised, unsupervised, or reinforcement learning methods.\n",
      "2. **Natural Language Processing (NLP):** Facilitates understanding and generating human language, crucial for chatbots and virtual assistants.\n",
      "3. **Computer Vision:** Allows agents to interpret visual data, used in applications like facial recognition and autonomous vehicles.\n",
      "\n",
      "**Environment Interaction:**\n",
      "AI agents operate in environments that can be virtual or physical. They perceive through sensors and act through effectors. Environments vary in characteristics:\n",
      "- **Static/Dynamic:** Static environments don't change during task execution; dynamic ones do.\n",
      "- **Deterministic/Stochastic:** Deterministic environments have predictable outcomes; stochastic ones involve randomness.\n",
      "- **Episodic/Continuous:** Episodic agents interact in discrete episodes; continuous agents interact without interruption.\n",
      "\n",
      "**Industry Applications:**\n",
      "- **Customer Service:** Chatbots for 24/7 support.\n",
      "- **Healthcare:** Virtual assistants for patient monitoring.\n",
      "- **Manufacturing:** Autonomous robots for assembly lines.\n",
      "- **Finance:** Agents for algorithmic trading and fraud detection.\n",
      "- **Transportation:** Self-driving cars for autonomous navigation.\n",
      "- **Supply Chain:** Logistics optimization through predictive analytics.\n",
      "\n",
      "**Challenges and Limitations:**\n",
      "- **Data Privacy:** Compliance with regulations like GDPR.\n",
      "- **Bias and Fairness:** Ensuring unbiased decision-making.\n",
      "- **Explainability:** Making AI decisions transparent.\n",
      "- **Scalability and Performance:** Handling large-scale operations efficiently.\n",
      "- **Ethical Considerations:** Addressing job displacement and accountability.\n",
      "\n",
      "**Real-World Examples:**\n",
      "- **IBM Watson Assistant:** Customer service chatbot.\n",
      "- **Virtual Nurse Assistants:** Monitoring patient conditions.\n",
      "- **Kuka Robots:** Manufacturing automation.\n",
      "- **Robo-Advisors:** Financial investment management.\n",
      "- **Tesla Autopilot:** Semi-autonomous driving.\n",
      "- **SAP Systems:** Supply chain optimization.\n",
      "\n",
      "**Future Trends:**\n",
      "- **Edge AI:** Local data processing for real-time decisions.\n",
      "- **Multi-Agent Systems:** Collaboration for complex tasks.\n",
      "- **Explainable AI (XAI):** Enhancing transparency and trust.\n",
      "- **Human-AI Collaboration:** Synergistic teamwork for better outcomes.\n",
      "\n",
      "**Conclusion:**\n",
      "AI agents are transformative across industries, offering automation, scalability, and continuous operation. Leveraging ML, NLP, and Computer Vision, they drive efficiency and innovation. Addressing challenges like privacy and bias is crucial for sustained adoption and ethical use.\n"
     ]
    }
   ],
   "source": [
    "print(teacher_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4a011c353aad42e09e6a6cee3fc3d04b",
      "1205aca6ec9b4148a18c92d79a4b00ac",
      "2fc0c165c45c449a8790e39ba530aec6",
      "4fc936aec5874b93b9a4b4b939136084",
      "ccc1cd2916144ef886b117be2a84f67c",
      "1894c88535644763abe61226932bf326",
      "412b87f43e314debbe69b3b4df758dde",
      "c2e0bb7c670c49f29aab0b06500d4d32",
      "9118f2ac02cf4605a7a535d839aefdaf",
      "bffefc424bce4865911ca2723fc3eb66",
      "5ab18d2f8b7c4f0b93213a736abbc9ad",
      "02f0ffbc7e3a469aadee74ceab693af6",
      "67ca533350bd45f18c422bb6f483a4cd",
      "43fe5b7d76694d6d9c9c69c950ff31a1",
      "1338cd9b0c01441b810cb4aa72ef9af5",
      "2a4ac50225eb427d8edf8ca8632b2358",
      "07369e2e2d2745aa94a58d86e7be10d4",
      "b71c62b78fa34a0585d3c7f7b1a98d8f",
      "753bbb43ce314e1d92b425808a5b4cbb",
      "395ae88e8b2c4b77a22938ed1a02db88",
      "95067393ff014fcf8152143af81c6fc6",
      "601cdba3474740a08d7f920edbee74ca",
      "2698b2faf1b84a30bd0164eec8610ee3",
      "3191528d8c6b4d0e87b712e04c23386b",
      "69fd26491ba44090a357c4ab7b89890f",
      "f7eac98ec7ed49198401718614c6996a",
      "bd8c0748dce742f795fb53460b403fdc",
      "b7ce278432f94059a592b53620da68e7",
      "8dbb1fcac92940e8ad18bd86e50909f9",
      "ca6d970bb70a4459b7a5e38a9a827f62",
      "4ecf20d40e19411aafa85180be6df6a9",
      "d138d76463b7460986e0a7e41c5afa9a",
      "e6f2d6591be74bf9a7a0317684750b43"
     ]
    },
    "id": "lytyYWvIYYda",
    "outputId": "30f72d32-5edc-4076-ed92-7e48edf9fcd8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a011c353aad42e09e6a6cee3fc3d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f0ffbc7e3a469aadee74ceab693af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2698b2faf1b84a30bd0164eec8610ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "8809025173ec4b9fae0d557ebc26309b",
      "7d8c7d4bace14b31abb1d9bf8dc1d91b",
      "f0cdea940eb9458cae9aaae72448430d",
      "42b3f238c78e4e1bb6f7078bfe393f6b",
      "cac1a89efa234ee585ac77e33b7a2d02",
      "088457b766d1478a8845df406b754869",
      "ed3eea8ebc8f4a978f9bbb6f8c8b9b4b",
      "948bd9f3c4a04e73ad6c99c95987ecaf",
      "667155148e28469cb715557081ae1260",
      "6c4fc0d5609f4b668faac7820c149b4a",
      "24f0f2302464405fb384894ea6ff7855"
     ]
    },
    "id": "1Iy6sYLAZqMS",
    "outputId": "08a3037a-2af6-4d0e-a6a5-d6f0d70cd1ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_data at 0x7d27ed0a0f40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function process_data at 0x7d27ed0a0f40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8809025173ec4b9fae0d557ebc26309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prepare datasets with Rationales(Reasoning)\n",
    "\n",
    "def process_data(example):\n",
    "\n",
    "  input_text = example[\"sentence\"]\n",
    "\n",
    "  reasoning = generate_model_reasoning(input_text)\n",
    "\n",
    "  label = example[\"label\"]\n",
    "\n",
    "  #Tokenizing input and rationale/reasoning\n",
    "\n",
    "  input_tokens = llm_tokenizer(input_text, truncation =\"longest_first\", padding = True, max_length = 256)\n",
    "\n",
    "  reasoning_tokens = llm_tokenizer(reasoning, truncation = \"longest_first\", padding = True, max_length = 256)\n",
    "\n",
    "  return {\n",
    "\n",
    "          \"input_ids\" : input_tokens[\"input_ids\"],\n",
    "\n",
    "          \"attention_mask\" : input_tokens[\"attention_mask\"],\n",
    "\n",
    "          \"labels\" : label,\n",
    "\n",
    "          \"reasoning_ids \" : reasoning_tokens[\"input_ids\"],\n",
    "\n",
    "          \"reasoning_mask\" : reasoning_tokens[\"attention_mask\"]\n",
    "  }\n",
    "\n",
    "processed_dataset = dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSv02GhUeCR-",
    "outputId": "fabaeb78-b24e-4e17-937c-863734005df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'reasoning_ids ', 'reasoning_mask'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "14665447d29648149aae98eaa838f969",
      "4c2475ab3b25497d8f0454dc7e888caa",
      "5d35ad753d8445a29ef5d06cf0c6b131",
      "dd55d2733c9f4f3e8f5627ca34bf4422",
      "57f39a52aa63494c839797e30ef89784",
      "bb91250724a547d0930112f86892ff18",
      "1102b859de814927aa06eb4f93fec11d",
      "daec4fa838cc476dbe35b4e126c610c4",
      "80b6ea899e954fc6990ccd3da1f77a96",
      "feca39b8b7724750ad6dac8b6866eef7",
      "664a35085cac4c27baf8d1be9c38330c"
     ]
    },
    "id": "pcWicpmemCcN",
    "outputId": "14949f68-9045-41dd-ed2d-f7c0a02da176"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14665447d29648149aae98eaa838f969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "processed_dataset.save_to_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEOZIyklmmg9",
    "outputId": "e0737cff-42f4-4c8e-b919-9d5546790382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'reasoning_ids ', 'reasoning_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "processed_dataset.load_from_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I5XY0xSEm0j9",
    "outputId": "7e3722d2-095d-4839-feb8-3d94f87b2dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.26.0) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2025.1.31)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.5.2\n",
      "    Uninstalling accelerate-1.5.2:\n",
      "      Successfully uninstalled accelerate-1.5.2\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "f28630912b504a57b388b9327eb8f6b5",
       "pip_warning": {
        "packages": [
         "accelerate"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaiazrex8\u001b[0m (\u001b[33mfaiazrex8-scaler\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250413_154402-1fdfrlte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faiazrex8-scaler/huggingface/runs/1fdfrlte' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/faiazrex8-scaler/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faiazrex8-scaler/huggingface' target=\"_blank\">https://wandb.ai/faiazrex8-scaler/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faiazrex8-scaler/huggingface/runs/1fdfrlte' target=\"_blank\">https://wandb.ai/faiazrex8-scaler/huggingface/runs/1fdfrlte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.625250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.538202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6304543813069662, metrics={'train_runtime': 95.0521, 'train_samples_per_second': 0.316, 'train_steps_per_second': 0.063, 'total_flos': 396722138160.0, 'train_loss': 0.6304543813069662, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Student Model To Understand the Rationale/Reasoning Generated/Drawn by the Teacher Model and also the Labels\n",
    "\n",
    "!pip install -U 'accelerate==0.26.0'\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accelerate.__version__\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "\n",
    "                                       output_dir = \"./results\",\n",
    "\n",
    "                                       eval_strategy = \"epoch\",\n",
    "\n",
    "                                       learning_rate = 5e-5,\n",
    "\n",
    "                                       per_device_train_batch_size = 8,\n",
    "\n",
    "                                       per_device_eval_batch_size = 8,\n",
    "\n",
    "                                       num_train_epochs = 3,\n",
    "\n",
    "                                       weight_decay = 0.01,\n",
    "\n",
    "                                       save_strategy = \"epoch\",\n",
    "\n",
    "                                       push_to_hub = False,\n",
    "\n",
    ")\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs = False, num_items_in_batch = None, **kwargs):\n",
    "\n",
    "    labels = inputs.pop(\"labels\")\n",
    "\n",
    "    reasoning_ids = inputs.pop(\"reasoning_ids\", None)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    label_loss = loss_function(outputs.logits, labels)\n",
    "\n",
    "    if reasoning_ids is not None:\n",
    "\n",
    "      reasoning_outputs = model(input_ids = reasoning_ids, attention_mask = inputs[\"attention_mask\"])\n",
    "\n",
    "      reasoning_loss = loss_function(reasoning_outputs.logits, reasoning_ids)\n",
    "\n",
    "      loss = label_loss + 0.5 * reasoning_loss\n",
    "\n",
    "    else:\n",
    "\n",
    "      loss = label_loss\n",
    "\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "student_model_trainer = MultiTaskTrainer(\n",
    "\n",
    "                                         model = student_llm_model,\n",
    "\n",
    "                                         args = training_arguments,\n",
    "\n",
    "                                         train_dataset = processed_dataset,\n",
    "\n",
    "                                         eval_dataset = processed_dataset,\n",
    "\n",
    "                                         data_collator = DataCollatorWithPadding(tokenizer = llm_tokenizer)\n",
    ")\n",
    "\n",
    "student_model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2gtDnEdIAGh",
    "outputId": "f6a72c23-6dfa-421a-cd25-0e4165fcc020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask', 'labels', 'reasoning_ids ', 'reasoning_mask'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RjmGX2JKYR1",
    "outputId": "78bf6604-9f1d-4b47-85d1-cdaf4c948da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation Complete! Student Model and Tokenzier Saved!\n"
     ]
    }
   ],
   "source": [
    "student_model_save_path = \"./result\"\n",
    "\n",
    "student_model_trainer.save_model(student_model_save_path)\n",
    "\n",
    "llm_tokenizer.save_pretrained(student_model_save_path)\n",
    "\n",
    "print(\"Distillation Complete! Student Model and Tokenzier Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plKlDTJgfzpS",
    "outputId": "b0ec378f-1dfe-4052-82d1-f430f1b789ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment class 1\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the Student Model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(student_model_save_path)\n",
    "\n",
    "llm_model_tokenizer = AutoTokenizer.from_pretrained(student_model_save_path)\n",
    "\n",
    "student_model.eval()\n",
    "\n",
    "def predict_sentiment(input_text):\n",
    "\n",
    "    input_tokens = llm_model_tokenizer(input_text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "      output_tokens = student_model(**input_tokens)\n",
    "\n",
    "      logits = output_tokens.logits\n",
    "\n",
    "      predicted_sentiment_class = torch.argmax(logits, dim = -1).item()\n",
    "\n",
    "    return predicted_sentiment_class\n",
    "\n",
    "\n",
    "#Testing the Student Model\n",
    "\n",
    "user_input_text = \"are more deeply thought through than in most right thinking films\"\n",
    "\n",
    "sentiment_prediction = predict_sentiment(user_input_text)\n",
    "\n",
    "print(\"Predicted sentiment class\", sentiment_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_5QgxOH05Pf",
    "outputId": "db372325-5e3c-4dbf-e1ff-5e6133e74dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the sentiment of the sentence \"I am very happy today!\" Hmm, let me think about this step by step. First, I know that sentiment analysis is about determining whether a piece of text is positive, negative, or neutral. \n",
      "\n",
      "Looking at the sentence, the key word here is \"happy.\" I remember that \"happy\" is a positive emotion. The word \"very\" is an intensifier, which makes the sentiment stronger. So, \"very happy\" would definitely lean towards a positive sentiment. \n",
      "\n",
      "I should also consider the structure of the sentence. It's a straightforward statement without any negative words or phrases that might indicate sarcasm or a hidden negative meaning. The exclamation mark at the end adds emphasis, which further supports the positive sentiment.\n",
      "\n",
      "Wait, could there be any other factors? Maybe the context, but since there's no additional context given, I have to base my analysis solely on the sentence itself. There's no indication of irony or any negative words that might flip the meaning. \n",
      "\n",
      "So, putting it all together, the presence of \"happy,\" intensified by \"very,\" and the exclamation mark all point towards a positive sentiment. I don't see any elements that would suggest otherwise. Therefore, I'm pretty confident that the sentiment is positive.\n",
      "</think>\n",
      "\n",
      "The sentiment of the sentence \"I am very happy today!\" is positive. The use of the word \"happy,\" intensified by \"very,\" and the exclamation mark all contribute to a strong positive sentiment. \n",
      "\n",
      "**Answer:** Positive\n"
     ]
    }
   ],
   "source": [
    "#Giving new input and testing\n",
    "\n",
    "model_test_prompt = \"Predict the sentiment for this input - I am very happy today!\"\n",
    "\n",
    "model_response = teacher_model.invoke(model_test_prompt)\n",
    "\n",
    "print(model_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zrYoZCf77AU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
